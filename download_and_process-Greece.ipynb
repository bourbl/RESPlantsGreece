{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This script downlads and extracts the original data of renewable power plant lists from the data sources, processes and merges them. It subsequently adds the geolocation for each power plant. Finally it saves the DataFrames as pickle-files. Make sure you run the download and process Notebook before the validation and output Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents \n",
    "\n",
    "* [1. Script setup](#1.-Script-setup)\n",
    "* [2. Settings](#2.-Settings)\n",
    "    * [2.1 Choose download option](#2.1-Choose-download-option)\n",
    "    * [2.2 Download function](#2.2-Download-function)\n",
    "    * [2.3 Setup translation dictionaries](#2.3-Setup-translation-dictionaries)\n",
    "* [3. Download and process per country](#3.-Download-and-process-per-country)\n",
    "    * [3.5 Greece](#3.5-Greece)\n",
    "        * [3.5.1 Download and read](#3.5.1-Download-and-read)\n",
    "        * [3.5.2 Translate column names](#3.5.2-Translate-column-names)\n",
    "        * [3.5.3 Standardize energy source information](#3.5.3-Standardize-energy-source-information)\n",
    "        * [3.5.4 Add information about municipalities](#3.5.4-Add-information-about-municipalities)\n",
    "        * [3.5.5 Geographical data](#3.5.5-Geographical-data)\n",
    "        * [3.5.6 Transform to latin alphabet and match municipalities to the GADM names using the Levenshtein distance](#3.5.6-Transform-to-latin-alphabet-and-match-municipalities-to-the-GADM-names-using-the-Levenshtein-distance)\n",
    "        * [3.5.7 Aggregate data by municipality and technology and assign geographical coordinates](#3.5.7-Aggregate-data-by-municipality-and-technology-and-assign-geographical-coordinates)\n",
    "        * [3.5.8 Save](#3.5.8-Save)\n",
    "* [Part 2: Validation and output](validation_and_output.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Script setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import posixpath\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "import utm  # for transforming geoinformation in the utm format\n",
    "import requests \n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%d %b %Y %H:%M:%S'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Create input and output folders if they don't exist\n",
    "os.makedirs('input/original_data', exist_ok=True)\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('output/renewable_power_plants', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Choose download option\n",
    "The original data can either be downloaded from the original data sources as specified below or from the opsd-Server. Default option is to download from the original sources as the aim of the project is to stay as close to original sources as possible. However, if problems with downloads e.g. due to changing urls occur, you can still run the script with the original data from the opsd_server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_from = 'original_sources'\n",
    "# download_from = 'opsd_server' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if download_from == 'opsd_server':\n",
    "\n",
    "    # Specify direction to original_data folder on the opsd data server\n",
    "    url_opsd = 'http://data.open-power-system-data.org/renewables_power_plants/'\n",
    "    version = '2017-02-16'\n",
    "    folder = '/original_data'\n",
    "    session = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_and_cache(url, session=None):\n",
    "    \"\"\"This function downloads a file into a folder called \n",
    "    original_data and returns the local filepath.\"\"\"\n",
    "    path = urllib.parse.urlsplit(url).path\n",
    "    filename = posixpath.basename(path)\n",
    "    filepath = \"input/original_data/\" + filename\n",
    "    print(url)\n",
    "\n",
    "    # check if file exists, if not download it\n",
    "    filepath = \"input/original_data/\" + filename\n",
    "    print(filepath)\n",
    "    if not os.path.exists(filepath):\n",
    "        if not session:\n",
    "            print('No session')\n",
    "            session = requests.session()\n",
    "        \n",
    "        print(\"Downloading file: \", filename)\n",
    "        r = session.get(url, stream=True)\n",
    "\n",
    "        chuncksize = 1024\n",
    "        with open(filepath, 'wb') as file:\n",
    "            for chunck in r.iter_content(chuncksize):\n",
    "                file.write(chunck)\n",
    "    else:\n",
    "        print(\"Using local file from\", filepath)\n",
    "    filepath = '' + filepath\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Setup translation dictionaries\n",
    "\n",
    "Column and value names of the original data sources will be translated to English and standardized across different sources. Standardized column names, e.g. \"electrical_capacity\" are required to merge data in one DataFrame.<br>\n",
    "The column and the value translation lists are provided in the input folder of the Data Package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get column translation list\n",
    "columnnames = pd.read_csv('input/column_translation_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get value translation list\n",
    "valuenames = pd.read_csv('input/value_translation_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download and process per country\n",
    "\n",
    "For one country after the other, the original data is downloaded, read, processed, translated, eventually georeferenced and saved. If respective files are already in the local folder, these will be utilized.\n",
    "To process the provided data [pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) is applied.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Greece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Download and read\n",
    "The data which will be processed below is provided by:\n",
    "\n",
    "**[resoffice.gr](https://www.resoffice.gr)** - Information Registry of Operating Stations (only for renewable energy and highly efficient co-generation plants) provided by the General Secretariat of Energy and Mineral Raw Materials of the Ministry of Environment and Energy of Greece. The data is provided by the **[Regulatory Authority for Energy](http://www.rae.gr/site/en_US/portal.csp)**, **[Operator of Electricity Market](http://www.lagie.gr/nc/en/home/)**, **[Independent Power Transmission Operator](http://www.admie.gr/nc/en/home/)** and **[Hellenic Electricity Distribution Network Operator S.A.](https://deddie.gr/en)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the table with all RES sources including large hydro\n",
    "if download_from == 'original_sources':\n",
    "    \n",
    "    url_GR = \"https://www.resoffice.gr/file/reg/stations_1.jsp\"\n",
    "\n",
    "else:\n",
    "    url_GR = (url_opsd + version + folder + '/GR/stations_1.jsp')\n",
    "\n",
    "    \n",
    "# Get data of renewables per project\n",
    "GR_df = pd.read_excel(download_and_cache(url_GR),\n",
    "                      encoding='UTF8')\n",
    "\n",
    "# drop nan rows\n",
    "GR_df.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Translate column names\n",
    "To standardise the DataFrame the original column names wil be translated and new english column names wil be assigned to the DataFrame. The unique column names are required to merge the DataFrame.<br>\n",
    "The column_translation_list is provided here as csv in the input folder. It is loaded in _2.3 Setup of translation dictionaries_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_GR = columnnames[(columnnames['country'] == 'GR') & (columnnames['data_source'] == 'resoffice.gr')].index\n",
    "column_dict_GR = columnnames.loc[idx_GR].set_index('original_name')['opsd_name'].to_dict()\n",
    "column_dict_GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Translate columnnames and drop the non useful ones\n",
    "GR_df.rename(columns = column_dict_GR, inplace=True)\n",
    "GR_df = GR_df[list(set(column_dict_GR.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add data source\n",
    "GR_df['data_source'] = 'resoffice.gr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the translation terms for Greece, create dictionary and show dictionary\n",
    "idx_GR = valuenames[(valuenames['country'] == 'GR') & (valuenames['data_source'] == 'resoffice.gr')].index\n",
    "value_dict_GR = valuenames.loc[idx_GR].set_index('original_name')['opsd_name'].to_dict()\n",
    "value_dict_GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all original value names by the OPSD value names\n",
    "GR_df.replace(value_dict_GR, inplace=True)\n",
    "\n",
    "# drop nan capacity rows\n",
    "GR_df.drop(GR_df[GR_df['electrical_capacity'].isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Standardize energy source information\n",
    "Assign the energy_source_level categories as well as technology and source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary in order to assign energy_source to its subtype\n",
    "energy_source_dict_GR = valuenames.loc[idx_GR].set_index('opsd_name')['energy_source_level_2'].to_dict()\n",
    "\n",
    "GR_df['energy_source_level_1'] = 'Renewable energy'\n",
    "\n",
    "GR_df['energy_source_level_2'] = GR_df['technology']\n",
    "GR_df['energy_source_level_2'].replace(energy_source_dict_GR, inplace=True)\n",
    "\n",
    "biomass_ind = GR_df[GR_df['technology']=='Biomass and biogas'].index\n",
    "GR_df.loc[biomass_ind, 'energy_source_level_2'] = 'Bioenergy'\n",
    "GR_df.loc[biomass_ind, 'energy_source_level_3'] = 'Biomass and biogas'\n",
    "\n",
    "GR_df.loc[biomass_ind, 'technology'] = np.NaN\n",
    "\n",
    "GR_df['source'] = GR_df['technology']\n",
    "GR_df.loc[biomass_ind, 'source'] = 'Biomass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 Add information about municipalities\n",
    "Since the original table contains geographical data merely aggregated at regional level, more detailed information is obtained by visiting the registry table of each project individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# spawn multiple asynchronous threads to reduce the waiting time for the server response\n",
    "# thread pool example taken from\n",
    "# https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example\n",
    "\n",
    "import concurrent.futures\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URLS = [\"https://www.resoffice.gr/file/reg/view.jsp?mas=\"+str(code_number) for code_number in GR_df['code_number']]\n",
    "MAX_WORKERS = 100  # it takes approx. 3 minutes for 100 threads\n",
    "\n",
    "# Retrieve a single page and report the URL and contents\n",
    "def load_url(url, timeout):\n",
    "    with urlopen(url, timeout=timeout) as conn:\n",
    "        return conn.read()\n",
    "\n",
    "data = {}\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_url = {executor.submit(load_url, url, 60): url for url in URLS}\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            page = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (url, exc))\n",
    "        else:\n",
    "            #print('%r page is %d bytes' % (url, len(page)))\n",
    "            pass\n",
    "            \n",
    "        # retrieve the municipality name\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "        table = soup.find_all('table', attrs={'class':'table'})\n",
    "        rows = table[1].find_all('tr')\n",
    "\n",
    "        for rid,row in enumerate(rows):\n",
    "            cols = row.find_all('td')  # contains the column values for each table row\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            if rid == 4:  # the fourth row contains the location information (\"Θέση εγκατάστασης:\")\n",
    "                try:\n",
    "                    # the third column holds the municipality name\n",
    "                    data.update({int(url.split('=')[1]): [ele for ele in cols if ele][3]})\n",
    "                except:\n",
    "                    #print('Could not append for page', url)\n",
    "                    data.update({int(url.split('=')[1]): np.NaN})\n",
    "                break\n",
    "                \n",
    "# append data to dataframe\n",
    "GR_df.set_index('code_number', inplace=True)\n",
    "GR_df['municipality'] = pd.Series(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.5 Geographical data\n",
    "\n",
    "Data can be downloaded from: **[Global Administrative Areas](http://www.gadm.org/)**\n",
    "<br /><br />License: \"This dataset is freely available for academic use and other non-commercial use. Redistribution, or commercial use is not allowed without prior permission. You are free to create maps and use the data in other ways for publication in academic journals, books, reports, etc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The shapefile format is used for extraction of the geographical information\n",
    "try: \n",
    "    import geopandas as gpd\n",
    "    shp = gpd.read_file(os.path.join('GRC_adm_shp', 'GRC_adm3.shp')).set_index('NAME_3')\n",
    "    centroids = pd.DataFrame(columns=['lon', 'lat'], index=shp.index)\n",
    "    centroids.index.name = 'name'\n",
    "    centroids['lon'] = [p.x for p in shp.centroid]\n",
    "    centroids['lat'] = [p.y for p in shp.centroid]\n",
    "except ImportError:\n",
    "    try:\n",
    "        print('No geopandas installed, better try the csv file')\n",
    "        centroids = pd.read_csv('municipality_centroids.csv', index_col='name')\n",
    "    except OSError:\n",
    "        raise OSError('Could not find a source for the municipality coordinates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.6 Transform to latin alphabet and match municipalities to the GADM names using the Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_greeklish(s):\n",
    "    '''\n",
    "    Transform a string from Greek to Latin alphabet and bring them to String1-String2 format.\n",
    "    Moreover try to transform from genitive to nominative clause, unfortunately there is no universal rule for that.\n",
    "    '''\n",
    "    \n",
    "    # dictionary to latin alphabet\n",
    "    greeklish = dict(zip(list('ΑΒΓΔΕΖΗΙΚΛΜΝΞΟΠΡΣΤΥΦΩΪ'), list('avgdeziiklmnxoprstyfoi')))\n",
    "    greeklish.update({'Θ': 'th', 'Χ': 'ch', 'Ψ': 'ps'})\n",
    "    \n",
    "    if isinstance(s, str):\n",
    "        \n",
    "        s = ''.join(str(e) for e in [greeklish[char] if char in greeklish.keys() else char for char in s ])\n",
    "        s = s.replace('oy', 'ou')\n",
    "        \n",
    "        # replace all separators with dash\n",
    "        p = re.compile(',|&|\\\\bkai\\\\b')\n",
    "        s = p.sub('-', s)\n",
    "        \n",
    "        # remove whitespaces before and after dashes\n",
    "        p = re.compile('\\s*-\\s*')\n",
    "        s = p.sub('-', s)\n",
    "        \n",
    "        # replace some greek terms that in GADM appear in English\n",
    "        d = {'kentrik': 'central', 'anat': 'east', 'vorei': 'north', 'noti': 'south', \n",
    "             'dytik': 'west', 'limni': 'lake', 'mikr': 'lesser'}\n",
    "        for greek in d.keys():\n",
    "            p = re.compile('\\w+')\n",
    "            startswith = re.compile(greek, re.I)\n",
    "            to_replace = [e for e in p.findall(s) if startswith.search(e)]\n",
    "            if to_replace:\n",
    "                p = re.compile(''.join(e+'|' for e in to_replace)[:-1])\n",
    "                s = p.sub(d[greek], s)\n",
    "                \n",
    "        #try to transform to nominative\n",
    "        toNominative = OrderedDict([('iou', 'io'), ('s', ''), ('ou', 'os'), ('on', 'a')])\n",
    "        for phrase in toNominative.keys():\n",
    "            p = re.compile('\\w+')\n",
    "            to_replace = [e for e in p.findall(s) if e.endswith(phrase)]\n",
    "            if to_replace:\n",
    "                for original in to_replace:\n",
    "                    p = re.compile(original)\n",
    "                    s = p.sub(original[:-len(phrase)]+toNominative[phrase], s)\n",
    "        \n",
    "        # cut off more than two municipalities\n",
    "        positions = [pos for pos, char in enumerate(s) if char == '-']\n",
    "        if len(positions) > 1:\n",
    "            s = s[:positions[1]]\n",
    "        \n",
    "        # remove whitespaces in head and tail\n",
    "        s = s.strip()\n",
    "        \n",
    "        s = s.title()\n",
    "        \n",
    "    return s\n",
    "\n",
    "GR_df['municipality'] = GR_df.municipality.apply(to_greeklish)\n",
    "engNames = [s for s in set(GR_df['municipality']) if isinstance(s, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_strings(string, string_list):\n",
    "    import Levenshtein  # https://github.com/ztane/python-Levenshtein/\n",
    "    match = np.array([Levenshtein.distance(string, s) for s in string_list])\n",
    "    return match.argmin(), match.min()\n",
    "\n",
    "# consider only the first out of the two locations for matching\n",
    "df_names_cut = {s: s.split('-')[0] \n",
    "                if isinstance(s, str) and not s.startswith('Agio')\n",
    "                else s for s in set(GR_df['municipality'])}\n",
    "\n",
    "\n",
    "toShpNamesDict = {df_name: [shp.index[compare_strings(df_name, centroids.index)[0]],\n",
    "                            shp.index[compare_strings(df_names_cut[df_name], centroids.index)[0]]]\n",
    "                  \n",
    "                           # pick the match with the least score\n",
    "                           [np.array([compare_strings(df_name, centroids.index)[1],\n",
    "                                      compare_strings(df_names_cut[df_name], centroids.index)[1]]).argmin()]\n",
    "                  \n",
    "                  for df_name in set(GR_df['municipality']) if isinstance(df_name, str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## add some changes manually when the algorithm fails\n",
    "toShpNamesDict.update({'Agia Anargyra-Kamateros': 'Agioi Anargyroi-Kamatero',\n",
    "                       'Athinaia': 'Athens',\n",
    "                       'Chalkidea': 'Chalcis',\n",
    "                       'Delfa': 'Delphi',\n",
    "                       'Distomos-Arachova': 'Distomo-Arachova-Antikyra',\n",
    "                       'Iera Poli Mesologgio': 'Missolonghi',\n",
    "                       'Iita': 'Ios',\n",
    "                       'Kerkyra': 'Corfu',\n",
    "                       'Loutrakio-Agia Theodora': 'Loutraki-Agioi Theodoroi',\n",
    "                       'Mantoudio-Lake': 'Mantoudi-Limni-Agia Anna',\n",
    "                       'Naxos-Lesser Kyklada': 'Naxos and Lesser Cyclades',\n",
    "                       'Patrea': 'Patras',\n",
    "                       'Peiraio': 'Piraeus',\n",
    "                       'Salamino': 'Salamis Island',\n",
    "                       'Thira': 'Santorini',\n",
    "                       'Thivaia': 'Thebes',\n",
    "                       'Troizinia': 'Troizinia-Methana',\n",
    "                       'Vari-Voula': 'Vari-Voula-Vouliagmeni'\n",
    "                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.7 Aggregate data by municipality and technology and assign geographical coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## group by municipality and source\n",
    "agg_func = {col: 'first' for col in GR_df.columns.difference(['municipality', 'source'])}\n",
    "agg_func.update({'electrical_capacity': 'sum'})\n",
    "GR_df = GR_df.replace(toShpNamesDict).groupby(['municipality', 'source']).agg(agg_func).reset_index(level=1)\n",
    "\n",
    "## add centroids\n",
    "GR_df['lon'] = centroids['lon']\n",
    "GR_df['lat'] = centroids['lat']\n",
    "\n",
    "# reset index to avoid duplicate indices\n",
    "GR_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.8 Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GR_df.to_pickle('GR_renewables.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and validation of the renewable power plants list as well as the creation of CSV/XLSX/SQLite files can be found in Part 2 of this script. It also generates a daily time series of cumulated installed capacities by energy source."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
